{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "resnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGzVzNEVp4KS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Cuda 혹은 cpu를 사용하시오.\n",
        "############Write Your Code Here############\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "device = 'cuda'\n",
        "############################################\n",
        "\n",
        "\n",
        "class Custom_Dataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        #입력으로 들어온 X의 pixel값들을 0-1사이로 normalize하고 X의 shape을 (FB,C,H,W)로 변경하여 저장하여 self.X,self.y에 저장.\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        X = X.astype('float32')\n",
        "        X /= 255.0\n",
        "\n",
        "        self.X = np.transpose(X, (0,3,1,2))\n",
        "        self.y = y\n",
        "        \n",
        "    def __len__(self):\n",
        "        result = 0\n",
        "        result = len(self.X)\n",
        "        return result\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        result_X,result_y = None,None\n",
        "        result_X = self.X[idx]\n",
        "        result_y = self.y[idx]\n",
        "        return result_X,result_y\n",
        "\n",
        "    \n",
        "#torch.nn을 사용하여 아래 함수들을 작성하시오. result는 nn.Layer중 하나이고 result를 반환함\n",
        "def batch_norm(dim,for_MLP=True):\n",
        "    #for_MLP가 True일 시 MLP를 위한 BN Layer를 반환하고 False일 시 CNN을 위한 BN Layer를 반환함.\n",
        "    if for_MLP:\n",
        "      result = nn.BatchNorm1d(dim)\n",
        "    else:\n",
        "      result = nn.BatchNorm2d(dim)\n",
        "    return result\n",
        "\n",
        "def fc_layer(in_dim,out_dim):\n",
        "    #Fully Connected Layer(Dense Layer)\n",
        "    result = torch.nn.Linear(in_dim, out_dim)\n",
        "    return result\n",
        "\n",
        "def conv_layer(in_ch,out_ch,kernel_size, stride=1, padding=0):\n",
        "    #Convolutional Layer for image\n",
        "    result = nn.Conv2d(in_ch, out_ch, kernel_size, stride=stride, padding=padding, bias=False)\n",
        "    return result\n",
        "\n",
        "def relu():\n",
        "    #ReLU function\n",
        "    result = nn.ReLU()\n",
        "    return result\n",
        "\n",
        "def flatten():\n",
        "    #Flatten the data\n",
        "    result = nn.Flatten()\n",
        "    return result\n",
        "\n",
        "#skip_connection(bn -> relu -> conv -> bn -> relu -> conv)를 따르는 Res_block.\n",
        "#change_res가 True인 res_block을 통과한 feature map은 resolution이 2배 작아지고 channel의 깊이는 2배로 증가함. ex) 32*8*8 -> 64*4*4\n",
        "#위의 경우에는 skip_connection의 dimension은 1*1 conv로 맞춰줌.\n",
        "#change_res가 False인 Res_block을 통과한 feature map은 resolution과 channel의 깊이는 그대로 유지됨. ex) 32*4*4 -> 32*4*4(20점)\n",
        "class Res_block(nn.Module):\n",
        "    def __init__(self, input_channel, change_res):\n",
        "        super(Res_block,self).__init__()\n",
        "        self.change_res = change_res\n",
        "        if change_res:\n",
        "            self.conv1 = conv_layer(input_channel, input_channel*2, kernel_size=3, stride=2, padding=1)\n",
        "            self.bn1= batch_norm(input_channel*2, for_MLP=False)\n",
        "            self.shortcut = nn.Sequential(\n",
        "                conv_layer(input_channel, input_channel*2, kernel_size=1, stride=2),\n",
        "                batch_norm(input_channel*2, for_MLP=False)\n",
        "            )\n",
        "            input_channel = input_channel * 2\n",
        "        else:\n",
        "            self.conv1 = conv_layer(input_channel, input_channel, 3, 1, 1) #in_ch, out_ch, kernel, stride, padding\n",
        "            self.bn1 = batch_norm(input_channel, for_MLP=False)\n",
        "            self.shortcut = nn.Sequential()\n",
        "        self.conv2 = conv_layer(input_channel, input_channel, 3, 1, 1)\n",
        "        self.bn2 = batch_norm(input_channel, for_MLP=False)   \n",
        "        self.relu = relu()\n",
        "    def forward(self,X):\n",
        "        out = self.relu(self.bn1(self.conv1(X)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        #print(out.size(), X.size(), self.shortcut(X).size())\n",
        "        out += self.shortcut(X)\n",
        "        out = self.relu(out)\n",
        "        X = out\n",
        "        return X\n",
        "\n",
        "    \n",
        "#Skip Connection을 이용하여 20개 이상의 layer를 가지고 테스트 셋에대하여 50% 이상의 성능을 주는 MLP.\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MLP,self).__init__()\n",
        "        self.flatten = flatten()\n",
        "        self.relu = relu()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "\n",
        "        self.linear1 = fc_layer(input_dim, 512)\n",
        "        self.linear2 = fc_layer(512, 256)\n",
        "        self.linear3 = fc_layer(256,256)\n",
        "        self.linear4 = fc_layer(256,128)\n",
        "        self.bn4 = batch_norm(128)\n",
        "        self.linear5 = fc_layer(128, 128)\n",
        "        self.linear6 = fc_layer(128, 64)\n",
        "        self.bn5 = batch_norm(64)\n",
        "        self.linear7 = fc_layer(64, 64)\n",
        "        self.last_layer = fc_layer(64, 10)\n",
        "\n",
        "\n",
        "    def forward(self,X):\n",
        "        X = self.flatten(X)\n",
        "        X = self.relu(self.linear1(X)) # 1\n",
        "        X = self.dropout(X)\n",
        "        X = self.relu(self.linear2(X)) # 2\n",
        "        X = self.dropout(X)\n",
        "        X = self.relu(self.linear3(X)) # 3\n",
        "        out = X\n",
        "        X = self.relu(self.linear3(X)) # 4\n",
        "        X = self.relu(self.linear3(X)) # 5\n",
        "        X = self.relu(self.linear3(X)) # 6\n",
        "        X = self.linear3(X) # 7\n",
        "        X += out\n",
        "        X = self.relu(X)\n",
        "\n",
        "        X = self.bn4(self.relu(self.linear4(X))) # 8\n",
        "        out = X\n",
        "        X = self.relu(self.linear5(X)) # 9\n",
        "        X = self.relu(self.linear5(X)) # 10\n",
        "        X = self.relu(self.linear5(X)) # 11\n",
        "        X = self.linear5(X) # 12\n",
        "        X += out\n",
        "        X = self.relu(X)\n",
        "\n",
        "        X = self.bn5(self.relu(self.linear6(X))) # 13\n",
        "        out = X\n",
        "        X = self.relu(self.linear7(X)) # 14\n",
        "        X = self.relu(self.linear7(X)) # 15\n",
        "        X = self.linear7(X) #16\n",
        "        X += out\n",
        "        X = self.relu(X)\n",
        "\n",
        "        X = self.relu(self.linear7(X)) # 17 \n",
        "        X = self.relu(self.linear7(X)) # 18\n",
        "        X = self.linear7(X) # 19\n",
        "        X += out\n",
        "        X = self.relu(X)\n",
        "\n",
        "        X = self.relu(self.last_layer(X)) # 20\n",
        "\n",
        "        return X\n",
        "        \n",
        "#Res_Block을 사용하여 테스트 셋에대한 70% 이상의 성능을 주는 CNN 모델을 만드시오.\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channel, class_number, block_number):\n",
        "        super(CNN,self).__init__()\n",
        "        self.conv1 = conv_layer(3, input_channel, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = batch_norm(input_channel,for_MLP=False)\n",
        "        self.layer1 = Res_block(input_channel, 0) \n",
        "        self.layer2 = Res_block(input_channel, 0)\n",
        "\n",
        "        self.layer3 = Res_block(input_channel, change_res=True) # residual block => depth * 2\n",
        "        self.layer4 = Res_block(input_channel*2, 0) \n",
        "\n",
        "        self.layer5 = Res_block(input_channel*2, change_res=True) # residual block => depth * 2\n",
        "        self.layer6 = Res_block(input_channel*4, 0)\n",
        "\n",
        "        self.layer7 = Res_block(input_channel*4, change_res=True) # residual block => depth * 2 \n",
        "        self.layer8 = Res_block(input_channel*8, 0)\n",
        "\n",
        "        self.linear = fc_layer(input_channel*8, class_number)  \n",
        "        self.relu = relu()\n",
        "    def forward(self,X):\n",
        "        out = self.relu(self.bn1(self.conv1(X)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "  \n",
        "        out = nn.functional.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        X = out\n",
        "\n",
        "        return X\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    accuracy = 0\n",
        "    total_example = 0\n",
        "    correct_example = 0\n",
        "    for data in loader:\n",
        "        x,y = data\n",
        "        x = torch.tensor(x, device = device)\n",
        "        y = torch.tensor(y, device = device)\n",
        "\n",
        "        output = model(x)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        \n",
        "        total_example += y.size(0)\n",
        "        correct_example += (predicted == y).sum().item()\n",
        "    accuracy = correct_example / total_example * 100\n",
        "\n",
        "    model.train()\n",
        "    return accuracy\n",
        "\n",
        "#epoch마다 train_loader에 있는 batch들을 사용하여 모델을 학습하고\n",
        "#epoch의 마지막 iteration에서는 모델의 validation accuracy를 확인하여 제일 높은 val. acc.를 가진 model을 best_model에 저장하고\n",
        "#val_acc에는 매 epoch마다 구해진 validation accuracy를 저장\n",
        "#running_loss에는 각각의 epoch에서 모든 batch의 loss를 다 더하여 저장\n",
        "def train(model, epoches, train_loader, val_loader, optimizer, criteria, scheduler): # scheduler parameter를 추가해주어 learning rate을 변화시켜주었다.\n",
        "    best_score = 0\n",
        "    best_model = None\n",
        "    batch_len = len(train_loader)\n",
        "    val_acc = []\n",
        "    for epoch in range(epoches):\n",
        "        running_loss = 0\n",
        "        for i,data in enumerate(train_loader):\n",
        "            x,y = data\n",
        "            x = torch.tensor(x, device = device)\n",
        "            y = torch.tensor(y, device = device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(x)\n",
        "            loss = criteria(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step() \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            #epoch의 마지막 iteration\n",
        "            if i % batch_len == batch_len-1:\n",
        "                print(f'{epoch+1}th iteration loss :',running_loss/batch_len)\n",
        "                running_loss = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                  correct = 0\n",
        "                  total = 0\n",
        "                  for i, data in enumerate(val_loader):\n",
        "                    val_x, val_y = data\n",
        "                    val_x = torch.tensor(val_x, device = device)\n",
        "                    val_y = torch.tensor(val_y, device = device)\n",
        "                    val_out = model(val_x)\n",
        "                    _, predicted = torch.max(val_out.data, 1)\n",
        "                    total += val_y.size(0)\n",
        "                    correct += (predicted == val_y).sum().item()\n",
        " \n",
        "\n",
        "                accuracy = 100 * correct / total\n",
        "                print(\"val acc :\",accuracy)\n",
        "                val_acc.append(accuracy)\n",
        "\n",
        "                if best_score < accuracy:\n",
        "                  best_score = accuracy\n",
        "                  best_model = deepcopy(model)\n",
        "\n",
        "    return best_model, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0r-0MEOrp4KZ",
        "outputId": "090175b6-5459-40c2-d6e3-cbbe4c867fdd"
      },
      "source": [
        "#Read the data\n",
        "trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True)\n",
        "testset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True)\n",
        "\n",
        "X_train, Y_train = trainset.data, np.array(trainset.targets)\n",
        "X_test, Y_test = testset.data, np.array(testset.targets)\n",
        "\n",
        "#앞서 정의한 Custom_Dataset과 DataLoader를 사용하여 train_loader,val_loader,test_loader를 정의하시오.\n",
        "#Batch_size는 본인의 컴퓨터 사향에 맞게 변경하면 됨. Validation Set으로 Train Set의 20%를 사용함.\n",
        "#Preprocessing\n",
        "train_loader = None\n",
        "val_loader = None\n",
        "test_loader = None\n",
        "batch_size = 128\n",
        "\n",
        "# data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "trainset.transform = transform_train\n",
        "testset.transform = transform_test\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1, stratify=Y_train)\n",
        "\n",
        "train_dataset = Custom_Dataset(X_train, y_train) # load train Dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_dataset = Custom_Dataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n",
        "test_dataset = Custom_Dataset(X_test, Y_test) # load test Dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, num_workers=2)\n",
        "\n",
        "\n",
        "#앞서 정의한 MLP,CNN을 사용하여 mlp_model,cnn_model을 정의.\n",
        "#Define the model\n",
        "mlp_model = None\n",
        "cnn_model = None\n",
        "mlp_model = MLP(32*32*3, 10)\n",
        "cnn_model = CNN(16, 10, 0)\n",
        "\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_mlp = torch.optim.SGD(mlp_model.parameters(), lr=0.1, momentum=0.9)\n",
        "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "scheduler_mlp = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_mlp, T_max=30) # use cosineannealing scheduler for MLP, CNN\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25)\n",
        "#scheduler_mlp = torch.optim.lr_scheduler.MultiStepLR(optimizer_mlp, milestones=[100, 200], gamma=0.1)\n",
        "\n",
        "mlp_model.to(device)\n",
        "cnn_model.to(device)\n",
        "\n",
        "#Train the model\n",
        "best_mlp = None\n",
        "mlp_val_acc = None\n",
        "best_cnn = None\n",
        "cnn_val_acc = None\n",
        "best_mlp, mlp_val_acc = train(mlp_model, 450, train_loader, val_loader, optimizer_mlp, criteria, scheduler_mlp)\n",
        "best_cnn, cnn_val_acc = train(cnn_model, 100, train_loader, val_loader, optimizer, criteria, scheduler)\n",
        "\n",
        "\n",
        "\n",
        "#Test Accuracy\n",
        "mlp_acc = None  \n",
        "cnn_acc = None \n",
        "mlp_acc = evaluate(best_mlp, test_loader)\n",
        "cnn_acc = evaluate(best_cnn, test_loader)\n",
        "print('MLP accuracy:',mlp_acc)\n",
        "print('CNN accuracy:',cnn_acc)\n",
        "\n",
        "\n",
        "#Validation Accuracy Plot\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(cnn_val_acc, label=\"CNN Acc\")\n",
        "plt.plot(mlp_val_acc, label=\"MLP Acc\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:277: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1th iteration loss : 2.301500082015991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:300: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:301: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val acc : 10.56\n",
            "2th iteration loss : 2.3019012749766388\n",
            "val acc : 11.06\n",
            "3th iteration loss : 2.301704881290277\n",
            "val acc : 10.7\n",
            "4th iteration loss : 2.3014594724003117\n",
            "val acc : 10.99\n",
            "5th iteration loss : 2.301380978605618\n",
            "val acc : 10.52\n",
            "6th iteration loss : 2.2998102655806862\n",
            "val acc : 11.22\n",
            "7th iteration loss : 2.2941185498770813\n",
            "val acc : 13.68\n",
            "8th iteration loss : 2.263877758964563\n",
            "val acc : 19.15\n",
            "9th iteration loss : 2.2139880223015247\n",
            "val acc : 22.08\n",
            "10th iteration loss : 2.174041961328671\n",
            "val acc : 24.25\n",
            "11th iteration loss : 2.1079120567431464\n",
            "val acc : 26.05\n",
            "12th iteration loss : 2.0462918041613154\n",
            "val acc : 28.94\n",
            "13th iteration loss : 2.0374445530553214\n",
            "val acc : 26.35\n",
            "14th iteration loss : 2.0171206286920906\n",
            "val acc : 28.26\n",
            "15th iteration loss : 1.9913805841257015\n",
            "val acc : 29.46\n",
            "16th iteration loss : 1.9729379087972183\n",
            "val acc : 30.19\n",
            "17th iteration loss : 1.9759961054347956\n",
            "val acc : 30.72\n",
            "18th iteration loss : 1.9521093159057081\n",
            "val acc : 30.65\n",
            "19th iteration loss : 1.9279842247216465\n",
            "val acc : 31.57\n",
            "20th iteration loss : 1.9243479167310575\n",
            "val acc : 32.39\n",
            "21th iteration loss : 1.9119772027475765\n",
            "val acc : 32.11\n",
            "22th iteration loss : 1.8931509519156557\n",
            "val acc : 33.64\n",
            "23th iteration loss : 1.8887383960687314\n",
            "val acc : 32.23\n",
            "24th iteration loss : 1.8859892093335477\n",
            "val acc : 33.16\n",
            "25th iteration loss : 1.8721051897865515\n",
            "val acc : 33.68\n",
            "26th iteration loss : 1.8651241989562306\n",
            "val acc : 34.8\n",
            "27th iteration loss : 1.858024581171834\n",
            "val acc : 33.68\n",
            "28th iteration loss : 1.856038288948254\n",
            "val acc : 35.33\n",
            "29th iteration loss : 1.8427653887782234\n",
            "val acc : 34.24\n",
            "30th iteration loss : 1.8454626551070534\n",
            "val acc : 35.19\n",
            "31th iteration loss : 1.8374130798224062\n",
            "val acc : 35.69\n",
            "32th iteration loss : 1.8216999635909692\n",
            "val acc : 35.74\n",
            "33th iteration loss : 1.820252885833716\n",
            "val acc : 34.88\n",
            "34th iteration loss : 1.8149492264555667\n",
            "val acc : 35.09\n",
            "35th iteration loss : 1.8072852600877658\n",
            "val acc : 35.6\n",
            "36th iteration loss : 1.8000492741124698\n",
            "val acc : 36.44\n",
            "37th iteration loss : 1.7912787709373255\n",
            "val acc : 35.21\n",
            "38th iteration loss : 1.7947163517101885\n",
            "val acc : 35.55\n",
            "39th iteration loss : 1.7965822189379805\n",
            "val acc : 36.91\n",
            "40th iteration loss : 1.7924642871363095\n",
            "val acc : 36.88\n",
            "41th iteration loss : 1.7795484256439698\n",
            "val acc : 37.87\n",
            "42th iteration loss : 1.707928993450567\n",
            "val acc : 37.74\n",
            "43th iteration loss : 1.6901957965887393\n",
            "val acc : 39.37\n",
            "44th iteration loss : 1.6863927426048742\n",
            "val acc : 39.11\n",
            "45th iteration loss : 1.675385784798156\n",
            "val acc : 40.44\n",
            "46th iteration loss : 1.6659465540712253\n",
            "val acc : 39.02\n",
            "47th iteration loss : 1.671313006657\n",
            "val acc : 39.54\n",
            "48th iteration loss : 1.6545171646264414\n",
            "val acc : 39.94\n",
            "49th iteration loss : 1.6543260245277478\n",
            "val acc : 39.56\n",
            "50th iteration loss : 1.6533304578580035\n",
            "val acc : 40.52\n",
            "51th iteration loss : 1.644054186610749\n",
            "val acc : 39.59\n",
            "52th iteration loss : 1.6364501737558041\n",
            "val acc : 40.22\n",
            "53th iteration loss : 1.6261897620301657\n",
            "val acc : 41.25\n",
            "54th iteration loss : 1.6232539480105757\n",
            "val acc : 41.87\n",
            "55th iteration loss : 1.6203950811117982\n",
            "val acc : 40.77\n",
            "56th iteration loss : 1.6213447217362376\n",
            "val acc : 40.37\n",
            "57th iteration loss : 1.6154917539499056\n",
            "val acc : 41.42\n",
            "58th iteration loss : 1.610682586511484\n",
            "val acc : 41.83\n",
            "59th iteration loss : 1.6036890954636156\n",
            "val acc : 42.41\n",
            "60th iteration loss : 1.6095250833529633\n",
            "val acc : 40.86\n",
            "61th iteration loss : 1.607084603736195\n",
            "val acc : 41.96\n",
            "62th iteration loss : 1.598985448432045\n",
            "val acc : 42.49\n",
            "63th iteration loss : 1.5904554764683636\n",
            "val acc : 43.03\n",
            "64th iteration loss : 1.584863740415238\n",
            "val acc : 41.42\n",
            "65th iteration loss : 1.5874283751740623\n",
            "val acc : 40.67\n",
            "66th iteration loss : 1.5930289315720336\n",
            "val acc : 41.98\n",
            "67th iteration loss : 1.5751271822962898\n",
            "val acc : 42.67\n",
            "68th iteration loss : 1.5810047513760699\n",
            "val acc : 44.03\n",
            "69th iteration loss : 1.566322837774746\n",
            "val acc : 42.66\n",
            "70th iteration loss : 1.5645066587307963\n",
            "val acc : 42.25\n",
            "71th iteration loss : 1.570880106081978\n",
            "val acc : 43.04\n",
            "72th iteration loss : 1.5624024064396136\n",
            "val acc : 43.57\n",
            "73th iteration loss : 1.55684080367652\n",
            "val acc : 42.22\n",
            "74th iteration loss : 1.5562340177286167\n",
            "val acc : 42.81\n",
            "75th iteration loss : 1.550057361301142\n",
            "val acc : 42.28\n",
            "76th iteration loss : 1.5508037188563484\n",
            "val acc : 43.28\n",
            "77th iteration loss : 1.5400511140640551\n",
            "val acc : 43.94\n",
            "78th iteration loss : 1.538520577616585\n",
            "val acc : 42.54\n",
            "79th iteration loss : 1.5412715642977828\n",
            "val acc : 42.7\n",
            "80th iteration loss : 1.5535929355377587\n",
            "val acc : 42.35\n",
            "81th iteration loss : 1.5394333307735455\n",
            "val acc : 43.28\n",
            "82th iteration loss : 1.5438825653764767\n",
            "val acc : 43.68\n",
            "83th iteration loss : 1.5377732999027727\n",
            "val acc : 43.66\n",
            "84th iteration loss : 1.523668302895543\n",
            "val acc : 42.3\n",
            "85th iteration loss : 1.5248882218290822\n",
            "val acc : 43.08\n",
            "86th iteration loss : 1.5196533896290838\n",
            "val acc : 44.39\n",
            "87th iteration loss : 1.522696744900542\n",
            "val acc : 43.88\n",
            "88th iteration loss : 1.5176403442510782\n",
            "val acc : 43.08\n",
            "89th iteration loss : 1.5153945673007172\n",
            "val acc : 42.95\n",
            "90th iteration loss : 1.5053704055353476\n",
            "val acc : 44.23\n",
            "91th iteration loss : 1.507309947531825\n",
            "val acc : 45.27\n",
            "92th iteration loss : 1.499372109437522\n",
            "val acc : 43.4\n",
            "93th iteration loss : 1.500606385282815\n",
            "val acc : 42.39\n",
            "94th iteration loss : 1.4971297057672812\n",
            "val acc : 43.96\n",
            "95th iteration loss : 1.4989050554391294\n",
            "val acc : 44.55\n",
            "96th iteration loss : 1.4953702135969655\n",
            "val acc : 44.39\n",
            "97th iteration loss : 1.492158351233973\n",
            "val acc : 42.52\n",
            "98th iteration loss : 1.4918457639103118\n",
            "val acc : 43.99\n",
            "99th iteration loss : 1.4830209873735714\n",
            "val acc : 44.18\n",
            "100th iteration loss : 1.4878421393446266\n",
            "val acc : 43.7\n",
            "101th iteration loss : 1.4831007193452634\n",
            "val acc : 44.35\n",
            "102th iteration loss : 1.480220227195813\n",
            "val acc : 43.81\n",
            "103th iteration loss : 1.4749999286267703\n",
            "val acc : 43.44\n",
            "104th iteration loss : 1.4786954455482313\n",
            "val acc : 44.24\n",
            "105th iteration loss : 1.456412941883928\n",
            "val acc : 45.91\n",
            "106th iteration loss : 1.4672049699119105\n",
            "val acc : 44.08\n",
            "107th iteration loss : 1.467279909136958\n",
            "val acc : 44.01\n",
            "108th iteration loss : 1.466664062521328\n",
            "val acc : 45.31\n",
            "109th iteration loss : 1.4589735665641272\n",
            "val acc : 44.53\n",
            "110th iteration loss : 1.4567866344421436\n",
            "val acc : 44.72\n",
            "111th iteration loss : 1.4572197099844106\n",
            "val acc : 43.62\n",
            "112th iteration loss : 1.4583902134301183\n",
            "val acc : 44.15\n",
            "113th iteration loss : 1.4395940669428426\n",
            "val acc : 44.34\n",
            "114th iteration loss : 1.4530631482791596\n",
            "val acc : 45.69\n",
            "115th iteration loss : 1.4427239723479786\n",
            "val acc : 44.9\n",
            "116th iteration loss : 1.4526932993635964\n",
            "val acc : 43.08\n",
            "117th iteration loss : 1.4455557775954468\n",
            "val acc : 45.24\n",
            "118th iteration loss : 1.4377991509513732\n",
            "val acc : 45.31\n",
            "119th iteration loss : 1.4327490215484326\n",
            "val acc : 44.83\n",
            "120th iteration loss : 1.4439103961371766\n",
            "val acc : 43.63\n",
            "121th iteration loss : 1.432345337761096\n",
            "val acc : 44.28\n",
            "122th iteration loss : 1.438102532118654\n",
            "val acc : 44.9\n",
            "123th iteration loss : 1.4351495439633013\n",
            "val acc : 44.85\n",
            "124th iteration loss : 1.4307272975056315\n",
            "val acc : 44.93\n",
            "125th iteration loss : 1.422855910782616\n",
            "val acc : 43.48\n",
            "126th iteration loss : 1.4262987032485084\n",
            "val acc : 44.86\n",
            "127th iteration loss : 1.4177658001835736\n",
            "val acc : 44.77\n",
            "128th iteration loss : 1.4160570043344467\n",
            "val acc : 46.17\n",
            "129th iteration loss : 1.4080815722767157\n",
            "val acc : 45.17\n",
            "130th iteration loss : 1.4169976844574317\n",
            "val acc : 44.89\n",
            "131th iteration loss : 1.4114686524906097\n",
            "val acc : 46.57\n",
            "132th iteration loss : 1.4188762263368113\n",
            "val acc : 45.55\n",
            "133th iteration loss : 1.411738807020096\n",
            "val acc : 46.64\n",
            "134th iteration loss : 1.4083555163666843\n",
            "val acc : 44.48\n",
            "135th iteration loss : 1.4043301759055629\n",
            "val acc : 45.76\n",
            "136th iteration loss : 1.4042679947405197\n",
            "val acc : 45.59\n",
            "137th iteration loss : 1.3954433404599516\n",
            "val acc : 46.11\n",
            "138th iteration loss : 1.4045122976120288\n",
            "val acc : 44.31\n",
            "139th iteration loss : 1.4072027511109177\n",
            "val acc : 44.18\n",
            "140th iteration loss : 1.4022322040015516\n",
            "val acc : 45.5\n",
            "141th iteration loss : 1.3977929685062493\n",
            "val acc : 45.38\n",
            "142th iteration loss : 1.3955866197427622\n",
            "val acc : 45.67\n",
            "143th iteration loss : 1.3925510206923317\n",
            "val acc : 45.45\n",
            "144th iteration loss : 1.3957793952557986\n",
            "val acc : 45.41\n",
            "145th iteration loss : 1.3762915366754744\n",
            "val acc : 44.8\n",
            "146th iteration loss : 1.3845436641583428\n",
            "val acc : 46.63\n",
            "147th iteration loss : 1.3872491894438626\n",
            "val acc : 45.68\n",
            "148th iteration loss : 1.3746565108101207\n",
            "val acc : 45.05\n",
            "149th iteration loss : 1.3710784984472841\n",
            "val acc : 45.52\n",
            "150th iteration loss : 1.3758631921804751\n",
            "val acc : 46.35\n",
            "151th iteration loss : 1.375249207210236\n",
            "val acc : 46.0\n",
            "152th iteration loss : 1.3855814114927103\n",
            "val acc : 45.31\n",
            "153th iteration loss : 1.3723869704590819\n",
            "val acc : 44.31\n",
            "154th iteration loss : 1.3709219671285953\n",
            "val acc : 46.32\n",
            "155th iteration loss : 1.360131182990516\n",
            "val acc : 46.86\n",
            "156th iteration loss : 1.3547884439127134\n",
            "val acc : 45.66\n",
            "157th iteration loss : 1.355659493242209\n",
            "val acc : 46.12\n",
            "158th iteration loss : 1.3527644000495205\n",
            "val acc : 45.56\n",
            "159th iteration loss : 1.364122678677495\n",
            "val acc : 46.73\n",
            "160th iteration loss : 1.3507249267718282\n",
            "val acc : 46.81\n",
            "161th iteration loss : 1.3480258437391288\n",
            "val acc : 45.85\n",
            "162th iteration loss : 1.3566739102141163\n",
            "val acc : 45.77\n",
            "163th iteration loss : 1.336104271130059\n",
            "val acc : 46.32\n",
            "164th iteration loss : 1.3485221813281123\n",
            "val acc : 45.84\n",
            "165th iteration loss : 1.3406773767532252\n",
            "val acc : 46.89\n",
            "166th iteration loss : 1.346907721159938\n",
            "val acc : 46.32\n",
            "167th iteration loss : 1.337971155635846\n",
            "val acc : 45.12\n",
            "168th iteration loss : 1.3388089657591555\n",
            "val acc : 46.32\n",
            "169th iteration loss : 1.3320726868443595\n",
            "val acc : 46.78\n",
            "170th iteration loss : 1.3378419022971448\n",
            "val acc : 45.66\n",
            "171th iteration loss : 1.3304562629602206\n",
            "val acc : 46.77\n",
            "172th iteration loss : 1.334886429789729\n",
            "val acc : 45.13\n",
            "173th iteration loss : 1.3246506010762418\n",
            "val acc : 46.02\n",
            "174th iteration loss : 1.3269004379979337\n",
            "val acc : 46.44\n",
            "175th iteration loss : 1.3279938263634143\n",
            "val acc : 45.2\n",
            "176th iteration loss : 1.3230347336290744\n",
            "val acc : 46.01\n",
            "177th iteration loss : 1.322416187856144\n",
            "val acc : 45.95\n",
            "178th iteration loss : 1.3267590915813994\n",
            "val acc : 46.35\n",
            "179th iteration loss : 1.307240903567963\n",
            "val acc : 46.79\n",
            "180th iteration loss : 1.3112518574102237\n",
            "val acc : 45.46\n",
            "181th iteration loss : 1.3033991330347883\n",
            "val acc : 45.23\n",
            "182th iteration loss : 1.3120413946267515\n",
            "val acc : 45.51\n",
            "183th iteration loss : 1.3103473506415615\n",
            "val acc : 46.5\n",
            "184th iteration loss : 1.3073330561573895\n",
            "val acc : 46.52\n",
            "185th iteration loss : 1.299257865348182\n",
            "val acc : 45.5\n",
            "186th iteration loss : 1.2994425677643797\n",
            "val acc : 46.5\n",
            "187th iteration loss : 1.300130472015649\n",
            "val acc : 47.26\n",
            "188th iteration loss : 1.3048312146061907\n",
            "val acc : 47.47\n",
            "189th iteration loss : 1.2965889026562627\n",
            "val acc : 46.13\n",
            "190th iteration loss : 1.3016454438431957\n",
            "val acc : 46.7\n",
            "191th iteration loss : 1.2898245931814274\n",
            "val acc : 46.17\n",
            "192th iteration loss : 1.2948553836383758\n",
            "val acc : 46.03\n",
            "193th iteration loss : 1.298717905919011\n",
            "val acc : 47.21\n",
            "194th iteration loss : 1.2891099449163808\n",
            "val acc : 46.29\n",
            "195th iteration loss : 1.2840657885463094\n",
            "val acc : 45.84\n",
            "196th iteration loss : 1.2819509106322218\n",
            "val acc : 46.72\n",
            "197th iteration loss : 1.2854269179292381\n",
            "val acc : 47.34\n",
            "198th iteration loss : 1.2855426771953082\n",
            "val acc : 45.97\n",
            "199th iteration loss : 1.2830603888240486\n",
            "val acc : 46.27\n",
            "200th iteration loss : 1.2812871076047612\n",
            "val acc : 46.57\n",
            "201th iteration loss : 1.2780388563204879\n",
            "val acc : 47.18\n",
            "202th iteration loss : 1.2686884258501827\n",
            "val acc : 46.98\n",
            "203th iteration loss : 1.2785961616534394\n",
            "val acc : 45.65\n",
            "204th iteration loss : 1.2785321089406363\n",
            "val acc : 46.34\n",
            "205th iteration loss : 1.2776271519950404\n",
            "val acc : 46.89\n",
            "206th iteration loss : 1.2783495751432716\n",
            "val acc : 46.92\n",
            "207th iteration loss : 1.2812141692295622\n",
            "val acc : 46.45\n",
            "208th iteration loss : 1.273579432560613\n",
            "val acc : 46.74\n",
            "209th iteration loss : 1.2744830298347596\n",
            "val acc : 46.51\n",
            "210th iteration loss : 1.267436751732811\n",
            "val acc : 47.08\n",
            "211th iteration loss : 1.2691156746099552\n",
            "val acc : 47.47\n",
            "212th iteration loss : 1.267905787728465\n",
            "val acc : 47.27\n",
            "213th iteration loss : 1.2652597278832627\n",
            "val acc : 45.32\n",
            "214th iteration loss : 1.2560411596450562\n",
            "val acc : 46.89\n",
            "215th iteration loss : 1.264504317658397\n",
            "val acc : 47.09\n",
            "216th iteration loss : 1.251146698150391\n",
            "val acc : 47.25\n",
            "217th iteration loss : 1.2562643200063859\n",
            "val acc : 46.48\n",
            "218th iteration loss : 1.24435718067157\n",
            "val acc : 45.67\n",
            "219th iteration loss : 1.2542224650184948\n",
            "val acc : 47.34\n",
            "220th iteration loss : 1.2561391750082802\n",
            "val acc : 47.23\n",
            "221th iteration loss : 1.2412799850058631\n",
            "val acc : 46.27\n",
            "222th iteration loss : 1.2419778299026978\n",
            "val acc : 46.43\n",
            "223th iteration loss : 1.2419490825634796\n",
            "val acc : 46.67\n",
            "224th iteration loss : 1.255921620340012\n",
            "val acc : 47.15\n",
            "225th iteration loss : 1.24821224342139\n",
            "val acc : 47.65\n",
            "226th iteration loss : 1.2380352589649894\n",
            "val acc : 47.08\n",
            "227th iteration loss : 1.239100140694993\n",
            "val acc : 45.91\n",
            "228th iteration loss : 1.2507849050025208\n",
            "val acc : 46.86\n",
            "229th iteration loss : 1.2367041929842184\n",
            "val acc : 47.51\n",
            "230th iteration loss : 1.2403473511290626\n",
            "val acc : 47.36\n",
            "231th iteration loss : 1.2339797029480004\n",
            "val acc : 46.27\n",
            "232th iteration loss : 1.2303377254702412\n",
            "val acc : 46.49\n",
            "233th iteration loss : 1.2213520958019903\n",
            "val acc : 46.93\n",
            "234th iteration loss : 1.23753080828883\n",
            "val acc : 47.88\n",
            "235th iteration loss : 1.2280299153190832\n",
            "val acc : 46.72\n",
            "236th iteration loss : 1.2355920211575664\n",
            "val acc : 45.09\n",
            "237th iteration loss : 1.219516833940634\n",
            "val acc : 47.29\n",
            "238th iteration loss : 1.2194027426524665\n",
            "val acc : 47.25\n",
            "239th iteration loss : 1.2188788041139182\n",
            "val acc : 47.04\n",
            "240th iteration loss : 1.2200270921658403\n",
            "val acc : 46.44\n",
            "241th iteration loss : 1.2098595526652596\n",
            "val acc : 47.28\n",
            "242th iteration loss : 1.2135961069085728\n",
            "val acc : 46.95\n",
            "243th iteration loss : 1.2085497672565448\n",
            "val acc : 48.52\n",
            "244th iteration loss : 1.22164288591653\n",
            "val acc : 46.14\n",
            "245th iteration loss : 1.2176028994706491\n",
            "val acc : 46.37\n",
            "246th iteration loss : 1.2136532595744147\n",
            "val acc : 46.93\n",
            "247th iteration loss : 1.2173419145349496\n",
            "val acc : 47.44\n",
            "248th iteration loss : 1.2115067902464456\n",
            "val acc : 47.5\n",
            "249th iteration loss : 1.2068824726171767\n",
            "val acc : 46.36\n",
            "250th iteration loss : 1.205606010013495\n",
            "val acc : 46.41\n",
            "251th iteration loss : 1.1966145712727556\n",
            "val acc : 47.61\n",
            "252th iteration loss : 1.199100106288069\n",
            "val acc : 47.63\n",
            "253th iteration loss : 1.1957342658941739\n",
            "val acc : 47.96\n",
            "254th iteration loss : 1.1969300531350766\n",
            "val acc : 46.05\n",
            "255th iteration loss : 1.2076038289755677\n",
            "val acc : 47.33\n",
            "256th iteration loss : 1.2076227002250501\n",
            "val acc : 46.88\n",
            "257th iteration loss : 1.1975625920981263\n",
            "val acc : 48.15\n",
            "258th iteration loss : 1.1970491371215723\n",
            "val acc : 46.6\n",
            "259th iteration loss : 1.1955875560117606\n",
            "val acc : 46.57\n",
            "260th iteration loss : 1.201527765764596\n",
            "val acc : 46.81\n",
            "261th iteration loss : 1.1967028379440308\n",
            "val acc : 47.95\n",
            "262th iteration loss : 1.1959040606745517\n",
            "val acc : 47.42\n",
            "263th iteration loss : 1.187508355885649\n",
            "val acc : 46.81\n",
            "264th iteration loss : 1.1893140361331904\n",
            "val acc : 47.18\n",
            "265th iteration loss : 1.1897050801176614\n",
            "val acc : 46.61\n",
            "266th iteration loss : 1.1841004631770686\n",
            "val acc : 47.8\n",
            "267th iteration loss : 1.177508082443152\n",
            "val acc : 46.55\n",
            "268th iteration loss : 1.1715104675140624\n",
            "val acc : 46.41\n",
            "269th iteration loss : 1.1782501120917714\n",
            "val acc : 47.91\n",
            "270th iteration loss : 1.1872387720753972\n",
            "val acc : 47.67\n",
            "271th iteration loss : 1.1777489278644038\n",
            "val acc : 47.79\n",
            "272th iteration loss : 1.1883486994920067\n",
            "val acc : 46.81\n",
            "273th iteration loss : 1.1861488468730792\n",
            "val acc : 46.84\n",
            "274th iteration loss : 1.172223217951985\n",
            "val acc : 46.47\n",
            "275th iteration loss : 1.1628581830106985\n",
            "val acc : 47.95\n",
            "276th iteration loss : 1.1723570100034768\n",
            "val acc : 47.44\n",
            "277th iteration loss : 1.168609847847265\n",
            "val acc : 47.03\n",
            "278th iteration loss : 1.1617634597296913\n",
            "val acc : 46.21\n",
            "279th iteration loss : 1.1737340612533373\n",
            "val acc : 46.74\n",
            "280th iteration loss : 1.1732640820570266\n",
            "val acc : 47.64\n",
            "281th iteration loss : 1.1669768630125272\n",
            "val acc : 46.89\n",
            "282th iteration loss : 1.1682389355696048\n",
            "val acc : 47.36\n",
            "283th iteration loss : 1.172865215772257\n",
            "val acc : 46.57\n",
            "284th iteration loss : 1.1532229819236852\n",
            "val acc : 48.26\n",
            "285th iteration loss : 1.1585733351616052\n",
            "val acc : 48.59\n",
            "286th iteration loss : 1.1555707812690126\n",
            "val acc : 46.21\n",
            "287th iteration loss : 1.1646802608197488\n",
            "val acc : 46.42\n",
            "288th iteration loss : 1.15266018820266\n",
            "val acc : 47.74\n",
            "289th iteration loss : 1.1518161580585444\n",
            "val acc : 47.86\n",
            "290th iteration loss : 1.1471305316248641\n",
            "val acc : 47.96\n",
            "291th iteration loss : 1.1407979222151419\n",
            "val acc : 48.07\n",
            "292th iteration loss : 1.143121615385476\n",
            "val acc : 46.74\n",
            "293th iteration loss : 1.1491006249056075\n",
            "val acc : 47.83\n",
            "294th iteration loss : 1.1462133180219145\n",
            "val acc : 48.0\n",
            "295th iteration loss : 1.1413508270876096\n",
            "val acc : 47.65\n",
            "296th iteration loss : 1.1463561284656343\n",
            "val acc : 46.67\n",
            "297th iteration loss : 1.1544956256406376\n",
            "val acc : 47.08\n",
            "298th iteration loss : 1.1471460583491828\n",
            "val acc : 46.99\n",
            "299th iteration loss : 1.1498836547421951\n",
            "val acc : 47.84\n",
            "300th iteration loss : 1.1427808866714135\n",
            "val acc : 47.26\n",
            "301th iteration loss : 1.1362255444161047\n",
            "val acc : 46.5\n",
            "302th iteration loss : 1.134572875195037\n",
            "val acc : 47.55\n",
            "303th iteration loss : 1.1440193931134746\n",
            "val acc : 48.08\n",
            "304th iteration loss : 1.1296379948957278\n",
            "val acc : 47.4\n",
            "305th iteration loss : 1.1356515415941184\n",
            "val acc : 47.09\n",
            "306th iteration loss : 1.1334457643115863\n",
            "val acc : 47.25\n",
            "307th iteration loss : 1.133363687001859\n",
            "val acc : 47.95\n",
            "308th iteration loss : 1.1372134952118602\n",
            "val acc : 48.21\n",
            "309th iteration loss : 1.1470205730523546\n",
            "val acc : 45.73\n",
            "310th iteration loss : 1.1533130672031318\n",
            "val acc : 46.05\n",
            "311th iteration loss : 1.1530134317973932\n",
            "val acc : 47.21\n",
            "312th iteration loss : 1.1381045166676798\n",
            "val acc : 46.9\n",
            "313th iteration loss : 1.1356742473456045\n",
            "val acc : 47.68\n",
            "314th iteration loss : 1.1420838918548804\n",
            "val acc : 47.31\n",
            "315th iteration loss : 1.132873683120496\n",
            "val acc : 47.17\n",
            "316th iteration loss : 1.135537413552927\n",
            "val acc : 46.79\n",
            "317th iteration loss : 1.1222535171828711\n",
            "val acc : 48.0\n",
            "318th iteration loss : 1.1301958351470411\n",
            "val acc : 46.82\n",
            "319th iteration loss : 1.1307328305305384\n",
            "val acc : 46.33\n",
            "320th iteration loss : 1.136850593189081\n",
            "val acc : 46.9\n",
            "321th iteration loss : 1.1214522728904748\n",
            "val acc : 46.83\n",
            "322th iteration loss : 1.1217684793396119\n",
            "val acc : 48.02\n",
            "323th iteration loss : 1.1147823366113365\n",
            "val acc : 47.12\n",
            "324th iteration loss : 1.129294058766228\n",
            "val acc : 46.43\n",
            "325th iteration loss : 1.1187890402425211\n",
            "val acc : 47.14\n",
            "326th iteration loss : 1.1131328826133435\n",
            "val acc : 47.77\n",
            "327th iteration loss : 1.1111177287924403\n",
            "val acc : 47.52\n",
            "328th iteration loss : 1.1090792566061782\n",
            "val acc : 46.39\n",
            "329th iteration loss : 1.111462035499061\n",
            "val acc : 47.08\n",
            "330th iteration loss : 1.1067785729234592\n",
            "val acc : 47.23\n",
            "331th iteration loss : 1.105628665453329\n",
            "val acc : 47.38\n",
            "332th iteration loss : 1.1048203836233852\n",
            "val acc : 46.29\n",
            "333th iteration loss : 1.102187953246668\n",
            "val acc : 47.53\n",
            "334th iteration loss : 1.10543865136826\n",
            "val acc : 47.13\n",
            "335th iteration loss : 1.0950719780815295\n",
            "val acc : 47.77\n",
            "336th iteration loss : 1.098688065053556\n",
            "val acc : 47.99\n",
            "337th iteration loss : 1.1129109653801963\n",
            "val acc : 46.72\n",
            "338th iteration loss : 1.112226563711136\n",
            "val acc : 46.81\n",
            "339th iteration loss : 1.1100222165592182\n",
            "val acc : 48.04\n",
            "340th iteration loss : 1.0971238799750234\n",
            "val acc : 48.74\n",
            "341th iteration loss : 1.086745648147961\n",
            "val acc : 47.39\n",
            "342th iteration loss : 1.088638826490591\n",
            "val acc : 47.06\n",
            "343th iteration loss : 1.0937284973863595\n",
            "val acc : 46.8\n",
            "344th iteration loss : 1.0935373066332394\n",
            "val acc : 47.48\n",
            "345th iteration loss : 1.0863818631004603\n",
            "val acc : 47.81\n",
            "346th iteration loss : 1.0954304116602522\n",
            "val acc : 47.12\n",
            "347th iteration loss : 1.08531733309499\n",
            "val acc : 45.87\n",
            "348th iteration loss : 1.0965801677383935\n",
            "val acc : 47.84\n",
            "349th iteration loss : 1.0867846516755442\n",
            "val acc : 48.25\n",
            "350th iteration loss : 1.0793844274819469\n",
            "val acc : 47.65\n",
            "351th iteration loss : 1.0785629715020664\n",
            "val acc : 46.71\n",
            "352th iteration loss : 1.082475622812399\n",
            "val acc : 46.82\n",
            "353th iteration loss : 1.086000382900238\n",
            "val acc : 48.17\n",
            "354th iteration loss : 1.0940652628676197\n",
            "val acc : 47.35\n",
            "355th iteration loss : 1.0894716204926609\n",
            "val acc : 47.77\n",
            "356th iteration loss : 1.0820916129377323\n",
            "val acc : 46.45\n",
            "357th iteration loss : 1.0786876270946222\n",
            "val acc : 48.13\n",
            "358th iteration loss : 1.086312420642414\n",
            "val acc : 47.1\n",
            "359th iteration loss : 1.0838931467586432\n",
            "val acc : 47.59\n",
            "360th iteration loss : 1.0693340095849082\n",
            "val acc : 47.07\n",
            "361th iteration loss : 1.0728975490640147\n",
            "val acc : 46.81\n",
            "362th iteration loss : 1.0679210953819105\n",
            "val acc : 47.27\n",
            "363th iteration loss : 1.0766248832495449\n",
            "val acc : 47.68\n",
            "364th iteration loss : 1.071336107322583\n",
            "val acc : 47.84\n",
            "365th iteration loss : 1.0695781808691665\n",
            "val acc : 46.6\n",
            "366th iteration loss : 1.072828828526762\n",
            "val acc : 47.98\n",
            "367th iteration loss : 1.0655007884144403\n",
            "val acc : 48.06\n",
            "368th iteration loss : 1.0718041290871252\n",
            "val acc : 48.28\n",
            "369th iteration loss : 1.0662385617582182\n",
            "val acc : 47.48\n",
            "370th iteration loss : 1.059496373985522\n",
            "val acc : 46.96\n",
            "371th iteration loss : 1.0569324563867368\n",
            "val acc : 47.77\n",
            "372th iteration loss : 1.0597650570610462\n",
            "val acc : 47.97\n",
            "373th iteration loss : 1.047164966313603\n",
            "val acc : 47.63\n",
            "374th iteration loss : 1.049031210212281\n",
            "val acc : 47.31\n",
            "375th iteration loss : 1.0468596619919848\n",
            "val acc : 48.1\n",
            "376th iteration loss : 1.0485396034801349\n",
            "val acc : 48.36\n",
            "377th iteration loss : 1.0573196932911493\n",
            "val acc : 48.66\n",
            "378th iteration loss : 1.0452926362665316\n",
            "val acc : 47.09\n",
            "379th iteration loss : 1.0539570340333275\n",
            "val acc : 47.75\n",
            "380th iteration loss : 1.0464256950460684\n",
            "val acc : 47.92\n",
            "381th iteration loss : 1.0537832034662509\n",
            "val acc : 47.74\n",
            "382th iteration loss : 1.0510575550432784\n",
            "val acc : 48.52\n",
            "383th iteration loss : 1.0530611404214805\n",
            "val acc : 47.23\n",
            "384th iteration loss : 1.0553136624086399\n",
            "val acc : 47.39\n",
            "385th iteration loss : 1.0511109882269423\n",
            "val acc : 47.99\n",
            "386th iteration loss : 1.0450107984649488\n",
            "val acc : 48.05\n",
            "387th iteration loss : 1.0476015994723993\n",
            "val acc : 48.28\n",
            "388th iteration loss : 1.0324845656800195\n",
            "val acc : 46.61\n",
            "389th iteration loss : 1.0408377875916113\n",
            "val acc : 47.69\n",
            "390th iteration loss : 1.047016251391877\n",
            "val acc : 48.5\n",
            "391th iteration loss : 1.0428230166435242\n",
            "val acc : 48.06\n",
            "392th iteration loss : 1.0262816875887375\n",
            "val acc : 47.68\n",
            "393th iteration loss : 1.0344632073713187\n",
            "val acc : 46.93\n",
            "394th iteration loss : 1.0301647557618139\n",
            "val acc : 47.1\n",
            "395th iteration loss : 1.0346718788527833\n",
            "val acc : 48.57\n",
            "396th iteration loss : 1.026847714623704\n",
            "val acc : 48.12\n",
            "397th iteration loss : 1.026222622813508\n",
            "val acc : 47.47\n",
            "398th iteration loss : 1.0291228943739454\n",
            "val acc : 47.37\n",
            "399th iteration loss : 1.021663859248542\n",
            "val acc : 47.73\n",
            "400th iteration loss : 1.021121445936136\n",
            "val acc : 48.07\n",
            "401th iteration loss : 1.0110918896647687\n",
            "val acc : 47.58\n",
            "402th iteration loss : 1.024722621273309\n",
            "val acc : 47.74\n",
            "403th iteration loss : 1.0268332944891323\n",
            "val acc : 47.05\n",
            "404th iteration loss : 1.0311448640716723\n",
            "val acc : 46.75\n",
            "405th iteration loss : 1.025427571500833\n",
            "val acc : 48.57\n",
            "406th iteration loss : 1.0265534598225603\n",
            "val acc : 47.68\n",
            "407th iteration loss : 1.0189633291369429\n",
            "val acc : 47.29\n",
            "408th iteration loss : 1.0115438812076094\n",
            "val acc : 47.91\n",
            "409th iteration loss : 1.0198910967610515\n",
            "val acc : 48.31\n",
            "410th iteration loss : 1.0134778321741489\n",
            "val acc : 48.56\n",
            "411th iteration loss : 1.0172807918950773\n",
            "val acc : 47.56\n",
            "412th iteration loss : 1.0074389552156005\n",
            "val acc : 47.75\n",
            "413th iteration loss : 1.023353277113491\n",
            "val acc : 48.11\n",
            "414th iteration loss : 1.0107393944606233\n",
            "val acc : 48.24\n",
            "415th iteration loss : 1.0012618274734424\n",
            "val acc : 48.09\n",
            "416th iteration loss : 1.0159167264597104\n",
            "val acc : 47.68\n",
            "417th iteration loss : 1.0206534668279532\n",
            "val acc : 46.42\n",
            "418th iteration loss : 1.0132094208424844\n",
            "val acc : 48.34\n",
            "419th iteration loss : 1.000328401597544\n",
            "val acc : 48.41\n",
            "420th iteration loss : 1.0084164433966811\n",
            "val acc : 47.09\n",
            "421th iteration loss : 1.0122692771613027\n",
            "val acc : 47.78\n",
            "422th iteration loss : 1.0113456447284443\n",
            "val acc : 48.53\n",
            "423th iteration loss : 0.9929386028847375\n",
            "val acc : 47.41\n",
            "424th iteration loss : 1.0180469574257969\n",
            "val acc : 47.79\n",
            "425th iteration loss : 1.0103574911245523\n",
            "val acc : 46.27\n",
            "426th iteration loss : 0.9944778079042038\n",
            "val acc : 47.68\n",
            "427th iteration loss : 0.9996846257307278\n",
            "val acc : 48.17\n",
            "428th iteration loss : 0.9970242367765774\n",
            "val acc : 48.53\n",
            "429th iteration loss : 1.0038543349256912\n",
            "val acc : 47.12\n",
            "430th iteration loss : 1.0157316827926393\n",
            "val acc : 47.58\n",
            "431th iteration loss : 0.9868287326047976\n",
            "val acc : 47.99\n",
            "432th iteration loss : 0.9947424243433407\n",
            "val acc : 47.76\n",
            "433th iteration loss : 0.9875350847792702\n",
            "val acc : 48.39\n",
            "434th iteration loss : 0.9941919177484969\n",
            "val acc : 47.36\n",
            "435th iteration loss : 0.9972078668804595\n",
            "val acc : 48.11\n",
            "436th iteration loss : 0.9911093165318425\n",
            "val acc : 48.15\n",
            "437th iteration loss : 0.9870084865024676\n",
            "val acc : 48.61\n",
            "438th iteration loss : 0.9889405723958732\n",
            "val acc : 48.36\n",
            "439th iteration loss : 0.9834482902155136\n",
            "val acc : 46.85\n",
            "440th iteration loss : 0.9864022023380755\n",
            "val acc : 47.76\n",
            "441th iteration loss : 0.998536295593737\n",
            "val acc : 47.34\n",
            "442th iteration loss : 0.9999738908804263\n",
            "val acc : 48.9\n",
            "443th iteration loss : 0.9750279296701327\n",
            "val acc : 47.32\n",
            "444th iteration loss : 0.9871143258798617\n",
            "val acc : 48.1\n",
            "445th iteration loss : 0.9804164719657776\n",
            "val acc : 47.86\n",
            "446th iteration loss : 0.979525397189509\n",
            "val acc : 48.32\n",
            "447th iteration loss : 0.9803063404826692\n",
            "val acc : 48.62\n",
            "448th iteration loss : 0.975865710086335\n",
            "val acc : 46.95\n",
            "449th iteration loss : 0.9767131255076716\n",
            "val acc : 47.23\n",
            "450th iteration loss : 0.9729104586683524\n",
            "val acc : 48.12\n",
            "1th iteration loss : 1.624007263122656\n",
            "val acc : 49.42\n",
            "2th iteration loss : 1.1842874520883773\n",
            "val acc : 63.09\n",
            "3th iteration loss : 0.9401470754093255\n",
            "val acc : 69.46\n",
            "4th iteration loss : 0.7781008588620268\n",
            "val acc : 70.41\n",
            "5th iteration loss : 0.6634473153196585\n",
            "val acc : 73.36\n",
            "6th iteration loss : 0.5737631757990621\n",
            "val acc : 75.36\n",
            "7th iteration loss : 0.4896657702069694\n",
            "val acc : 77.33\n",
            "8th iteration loss : 0.42225551124388416\n",
            "val acc : 76.62\n",
            "9th iteration loss : 0.35609262938887926\n",
            "val acc : 77.76\n",
            "10th iteration loss : 0.2988158183309217\n",
            "val acc : 77.5\n",
            "11th iteration loss : 0.25486248641158826\n",
            "val acc : 77.81\n",
            "12th iteration loss : 0.2005315900991519\n",
            "val acc : 76.68\n",
            "13th iteration loss : 0.18139154625872073\n",
            "val acc : 77.2\n",
            "14th iteration loss : 0.14384170947745203\n",
            "val acc : 77.4\n",
            "15th iteration loss : 0.12325671613335419\n",
            "val acc : 77.41\n",
            "16th iteration loss : 0.10364096399289541\n",
            "val acc : 76.09\n",
            "17th iteration loss : 0.0844838869528839\n",
            "val acc : 78.04\n",
            "18th iteration loss : 0.06566250916368092\n",
            "val acc : 77.95\n",
            "19th iteration loss : 0.06152202223781675\n",
            "val acc : 77.75\n",
            "20th iteration loss : 0.06177472087224356\n",
            "val acc : 76.15\n",
            "21th iteration loss : 0.06268134332907657\n",
            "val acc : 77.38\n",
            "22th iteration loss : 0.03648223958540553\n",
            "val acc : 78.28\n",
            "23th iteration loss : 0.03394871482926126\n",
            "val acc : 77.6\n",
            "24th iteration loss : 0.037062797110718186\n",
            "val acc : 77.16\n",
            "25th iteration loss : 0.032798166907862566\n",
            "val acc : 77.42\n",
            "26th iteration loss : 0.023069995906599722\n",
            "val acc : 78.71\n",
            "27th iteration loss : 0.028871298011484595\n",
            "val acc : 78.08\n",
            "28th iteration loss : 0.027323405400808222\n",
            "val acc : 77.71\n",
            "29th iteration loss : 0.018271261316236145\n",
            "val acc : 78.62\n",
            "30th iteration loss : 0.011197646406067078\n",
            "val acc : 79.09\n",
            "31th iteration loss : 0.008376660264326265\n",
            "val acc : 79.57\n",
            "32th iteration loss : 0.003983837689054362\n",
            "val acc : 79.58\n",
            "33th iteration loss : 0.0031413541052175446\n",
            "val acc : 79.77\n",
            "34th iteration loss : 0.0015004803266632147\n",
            "val acc : 79.58\n",
            "35th iteration loss : 0.0006041889183730751\n",
            "val acc : 79.92\n",
            "36th iteration loss : 0.00039832297565976007\n",
            "val acc : 79.88\n",
            "37th iteration loss : 0.00031286749310293846\n",
            "val acc : 79.83\n",
            "38th iteration loss : 0.00022299211250995427\n",
            "val acc : 79.94\n",
            "39th iteration loss : 0.00023245365550225868\n",
            "val acc : 79.98\n",
            "40th iteration loss : 0.0003551826627696578\n",
            "val acc : 79.99\n",
            "41th iteration loss : 0.0002216482934019991\n",
            "val acc : 79.93\n",
            "42th iteration loss : 0.0003052625774792057\n",
            "val acc : 80.16\n",
            "43th iteration loss : 0.00017077013578386505\n",
            "val acc : 80.17\n",
            "44th iteration loss : 0.00014307372529206934\n",
            "val acc : 80.09\n",
            "45th iteration loss : 0.00012254634586828813\n",
            "val acc : 80.1\n",
            "46th iteration loss : 0.00011632274687760688\n",
            "val acc : 80.19\n",
            "47th iteration loss : 0.00010221779567232101\n",
            "val acc : 80.03\n",
            "48th iteration loss : 0.00010301588001904527\n",
            "val acc : 80.05\n",
            "49th iteration loss : 8.749431974730903e-05\n",
            "val acc : 80.07\n",
            "50th iteration loss : 8.436573740435127e-05\n",
            "val acc : 80.13\n",
            "51th iteration loss : 9.482337580951928e-05\n",
            "val acc : 80.08\n",
            "52th iteration loss : 9.189677228033933e-05\n",
            "val acc : 80.0\n",
            "53th iteration loss : 9.136091740945126e-05\n",
            "val acc : 80.16\n",
            "54th iteration loss : 8.794509584624201e-05\n",
            "val acc : 80.14\n",
            "55th iteration loss : 8.329669237163341e-05\n",
            "val acc : 79.94\n",
            "56th iteration loss : 8.345276429795827e-05\n",
            "val acc : 79.99\n",
            "57th iteration loss : 8.217409814618624e-05\n",
            "val acc : 80.13\n",
            "58th iteration loss : 7.082227204864757e-05\n",
            "val acc : 80.05\n",
            "59th iteration loss : 8.029341424902575e-05\n",
            "val acc : 80.08\n",
            "60th iteration loss : 5.711288696429532e-05\n",
            "val acc : 80.05\n",
            "61th iteration loss : 5.477198684469893e-05\n",
            "val acc : 79.99\n",
            "62th iteration loss : 6.098122947022989e-05\n",
            "val acc : 80.02\n",
            "63th iteration loss : 9.677142206085797e-05\n",
            "val acc : 80.1\n",
            "64th iteration loss : 7.28085387716698e-05\n",
            "val acc : 80.19\n",
            "65th iteration loss : 6.029405686043344e-05\n",
            "val acc : 80.3\n",
            "66th iteration loss : 5.512692976004508e-05\n",
            "val acc : 80.29\n",
            "67th iteration loss : 6.14118337612257e-05\n",
            "val acc : 80.24\n",
            "68th iteration loss : 7.073845765448343e-05\n",
            "val acc : 80.26\n",
            "69th iteration loss : 5.5006815385243674e-05\n",
            "val acc : 80.17\n",
            "70th iteration loss : 4.65030773655006e-05\n",
            "val acc : 80.2\n",
            "71th iteration loss : 5.730930971271454e-05\n",
            "val acc : 80.22\n",
            "72th iteration loss : 4.7807971805885667e-05\n",
            "val acc : 80.16\n",
            "73th iteration loss : 4.970399554869405e-05\n",
            "val acc : 80.25\n",
            "74th iteration loss : 4.2487918196860926e-05\n",
            "val acc : 80.21\n",
            "75th iteration loss : 5.2454983977342066e-05\n",
            "val acc : 80.19\n",
            "76th iteration loss : 4.006261451504775e-05\n",
            "val acc : 80.2\n",
            "77th iteration loss : 4.041230105661535e-05\n",
            "val acc : 80.1\n",
            "78th iteration loss : 4.1357756105227715e-05\n",
            "val acc : 80.09\n",
            "79th iteration loss : 3.5865718079633234e-05\n",
            "val acc : 80.08\n",
            "80th iteration loss : 4.395867263127049e-05\n",
            "val acc : 80.07\n",
            "81th iteration loss : 3.601122259428253e-05\n",
            "val acc : 80.17\n",
            "82th iteration loss : 3.7067668054020126e-05\n",
            "val acc : 80.17\n",
            "83th iteration loss : 3.4029538617472626e-05\n",
            "val acc : 80.18\n",
            "84th iteration loss : 3.587350146410053e-05\n",
            "val acc : 80.25\n",
            "85th iteration loss : 3.5824287383678684e-05\n",
            "val acc : 80.21\n",
            "86th iteration loss : 5.517996569556627e-05\n",
            "val acc : 80.41\n",
            "87th iteration loss : 4.6341071621149525e-05\n",
            "val acc : 80.32\n",
            "88th iteration loss : 3.772904676643579e-05\n",
            "val acc : 80.33\n",
            "89th iteration loss : 3.0574950791322426e-05\n",
            "val acc : 80.37\n",
            "90th iteration loss : 3.361894032306922e-05\n",
            "val acc : 80.34\n",
            "91th iteration loss : 2.9550730338645206e-05\n",
            "val acc : 80.32\n",
            "92th iteration loss : 3.534166101855979e-05\n",
            "val acc : 80.22\n",
            "93th iteration loss : 2.8453306317322758e-05\n",
            "val acc : 80.27\n",
            "94th iteration loss : 3.2357724242243104e-05\n",
            "val acc : 80.24\n",
            "95th iteration loss : 3.014718398577379e-05\n",
            "val acc : 80.24\n",
            "96th iteration loss : 2.934109342111598e-05\n",
            "val acc : 80.23\n",
            "97th iteration loss : 3.4609753322420205e-05\n",
            "val acc : 80.21\n",
            "98th iteration loss : 4.0773117914435564e-05\n",
            "val acc : 80.16\n",
            "99th iteration loss : 4.549509480458159e-05\n",
            "val acc : 80.24\n",
            "100th iteration loss : 4.0031294009185484e-05\n",
            "val acc : 80.32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MLP accuracy: 53.16\n",
            "CNN accuracy: 80.45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6ec5a7c390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}